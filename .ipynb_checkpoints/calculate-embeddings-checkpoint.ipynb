{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers --upgrade\n",
    "# !pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from xml.dom import minidom\n",
    "\n",
    "from transformers import BertModel, BertTokenizer, BloomTokenizerFast, BloomModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#Timing\n",
    "from IPython.display import clear_output\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to parse xml\n",
    "def parseXML(filename, isInclude):\n",
    "    abstracts = []\n",
    "    tags = []\n",
    "    xmldoc = minidom.parse(filename)\n",
    "    itemlist = xmldoc.getElementsByTagName('abstract')\n",
    "    for node in xmldoc.getElementsByTagName('abstract'):\n",
    "        abstract = node.getElementsByTagName('style')[0].firstChild.nodeValue\n",
    "        abstracts.append(abstract)\n",
    "        tags.append(isInclude)  \n",
    "    return abstracts, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Find simple (tfidf) embeddings\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_simple_embeddings\u001b[39m(abstracts):\n\u001b[0;32m      4\u001b[0m     tfidfVectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Find simple (tfidf) embeddings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def get_simple_embeddings(abstracts):\n",
    "    tfidfVectorizer = TfidfVectorizer(stop_words=\"english\", max_features=768)\n",
    "    embeddings = tfidfVectorizer.fit_transform(abstracts).todense().tolist()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find scibert embeddings\n",
    "def get_scibert_embeddings(abstracts):\n",
    "    # Load scibert\n",
    "    scibert_model = BertModel.from_pretrained(\"allenai/scibert_scivocab_uncased\",\n",
    "                                      output_hidden_states=True)\n",
    "    scibert_tokenizer = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "    print('scibert_tokenizer is type:', type(scibert_tokenizer))\n",
    "    print('    scibert_model is type:', type(scibert_model))\n",
    "    \n",
    "    embeddings = []\n",
    "    length = len(abstracts.tolist())\n",
    "    index = 0\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "    for sentence in abstracts.tolist():\n",
    "        clear_output(wait=True)\n",
    "        index += 1\n",
    "        sen_emb = get_scibert_embedding(scibert_model, scibert_tokenizer, sentence)\n",
    "        embeddings.append(sen_emb)\n",
    "\n",
    "        stop = timeit.default_timer()\n",
    "\n",
    "        if (index/length*100) < 1:\n",
    "            expected_time = \"Calculating...\"\n",
    "\n",
    "        else:\n",
    "            time_perc = timeit.default_timer()\n",
    "            expected_time = np.round( (time_perc-start) /(index/length) /60,2)\n",
    "\n",
    "        print(index, length)\n",
    "        print(expected_time)\n",
    "        \n",
    "# Find bloom embeddings\n",
    "def get_bloom_embeddings(abstracts):\n",
    "    # Load scibert\n",
    "    bloom_tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-350m\")\n",
    "    bloom_model = BloomModel.from_pretrained(\"bigscience/bloom-350m\")\n",
    "\n",
    "    print('bloom_tokenizer is type:', type(bloom_tokenizer))\n",
    "    print('bloom_model is type:', type(bloom_model))\n",
    "    \n",
    "    embeddings = []\n",
    "    length = len(abstracts.tolist())\n",
    "    index = 0\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "    for sentence in abstracts.tolist():\n",
    "        clear_output(wait=True)\n",
    "        index += 1\n",
    "        sen_emb = get_bloom_embedding(bloom_model, bloom_tokenizer, sentence)\n",
    "        embeddings.append(sen_emb)\n",
    "\n",
    "        stop = timeit.default_timer()\n",
    "\n",
    "        if (index/length*100) < 1:\n",
    "            expected_time = \"Calculating...\"\n",
    "\n",
    "        else:\n",
    "            time_perc = timeit.default_timer()\n",
    "            expected_time = np.round( (time_perc-start) /(index/length) /60,2)\n",
    "\n",
    "        print(index, length)\n",
    "        print(expected_time)\n",
    "\n",
    "# Function to get document embedding\n",
    "def get_scibert_embedding(model, tokenizer, text):\n",
    "\n",
    "    # Encode with special tokens ([CLS] and [SEP], returning pytorch tensors\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,\n",
    "                        truncation=True,\n",
    "                        max_length=512,\n",
    "                        add_special_tokens = True,\n",
    "                        return_tensors = 'pt'\n",
    "                )\n",
    "    input_ids = encoded_dict['input_ids']  \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()   \n",
    "    \n",
    "    # Run through BERT\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        # Extract hidden states\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    # Select the embeddings\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "    # Calculate average of token vectors\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    # Convert to np array\n",
    "    sentence_embedding = sentence_embedding.detach().numpy()\n",
    "    print(\"sentence embedding shape:\", sentence_embedding.shape)\n",
    "\n",
    "    return sentence_embedding\n",
    "\n",
    "def get_bloom_embedding(model, tokenizer, text):\n",
    "\n",
    "    # Encode with special tokens ([CLS] and [SEP], returning pytorch tensors\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,\n",
    "                        truncation=True,\n",
    "                        max_length=512,\n",
    "                        add_special_tokens = True,\n",
    "                        return_tensors = 'pt'\n",
    "                )\n",
    "    input_ids = encoded_dict['input_ids']  \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()   \n",
    "    \n",
    "    # Run through Bloom\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        # Extract hidden states\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "\n",
    "    # Select the word embeddings on the last layer\n",
    "    token_vecs = last_hidden_states[0]\n",
    "    print(\"Token vecs:\", token_vecs.shape)\n",
    "    # Calculate average of token vectors/word embeddings\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    # Convert to np array\n",
    "    sentence_embedding = sentence_embedding.detach().numpy()\n",
    "    print(\"sentence embedding shape:\", sentence_embedding.shape)\n",
    "\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embeddings(name, method):\n",
    "    # Parse XML\n",
    "    abstractsInclude, tagsInclude = parseXML(name + '/' + name + 'Include.xml', 1)\n",
    "    abstractsExclude, tagsExclude = parseXML(name + '/' + name + 'Exclude.xml', 0)\n",
    "    df = pd.DataFrame(list(zip(tagsInclude + tagsExclude, abstractsInclude + abstractsExclude)), columns =['code', 'abstract'])\n",
    "\n",
    "    if method == \"simple\":\n",
    "        df['embeddings'] = get_simple_embeddings(df['abstract'])\n",
    "    if method == \"scibert\":\n",
    "        df['embeddings'] = get_scibert_embeddings(df['abstract'])\n",
    "    if method == \"bloom-350m\":\n",
    "        df['embeddings'] = get_bloom_embeddings(df['abstract'])\n",
    "    \n",
    "    # Save dataframe to prevent recalculation\n",
    "    df.to_pickle(\"./\" + name + \"/\" + name + \"-embeddings-\" + method + \".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token vecs: torch.Size([193, 1024])\n",
      "sentence embedding shape: (1024,)\n",
      "14 602\n",
      "35.46\n"
     ]
    }
   ],
   "source": [
    "# method - Select from [\"simple\", \"scibert\", \"bloom\"]\n",
    "method = \"bloom-350m\"\n",
    "\n",
    "names = [\"cellulitis\", \"copper\", \"search\", \"uti\", \"overdiagnosis\"]\n",
    "# names = [\"copper\"]\n",
    "\n",
    "for name in names:\n",
    "    calculate_embeddings(name, method)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "system",
   "language": "python",
   "name": "system"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
