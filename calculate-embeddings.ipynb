{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers --upgrade\n",
    "# !pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Connor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from xml.dom import minidom\n",
    "\n",
    "from transformers import BertModel, BertTokenizer, BloomTokenizerFast, BloomModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#Timing\n",
    "from IPython.display import clear_output\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to parse xml\n",
    "def parseXML(filename, isInclude):\n",
    "    abstracts = []\n",
    "    tags = []\n",
    "    xmldoc = minidom.parse(filename)\n",
    "    itemlist = xmldoc.getElementsByTagName('abstract')\n",
    "    for node in xmldoc.getElementsByTagName('abstract'):\n",
    "        abstract = node.getElementsByTagName('style')[0].firstChild.nodeValue\n",
    "        abstracts.append(abstract)\n",
    "        tags.append(isInclude)  \n",
    "    return abstracts, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find simple (tfidf) embeddings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def get_simple_embeddings(abstracts):\n",
    "    tfidfVectorizer = TfidfVectorizer(stop_words=\"english\", max_features=768)\n",
    "    embeddings = tfidfVectorizer.fit_transform(abstracts).todense().tolist()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find scibert embeddings\n",
    "def get_scibert_embeddings(abstracts):\n",
    "    # Load scibert\n",
    "    scibert_model = BertModel.from_pretrained(\"allenai/scibert_scivocab_uncased\",\n",
    "                                      output_hidden_states=True)\n",
    "    scibert_tokenizer = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "    print('scibert_tokenizer is type:', type(scibert_tokenizer))\n",
    "    print('    scibert_model is type:', type(scibert_model))\n",
    "    \n",
    "    embeddings = []\n",
    "    length = len(abstracts.tolist())\n",
    "    index = 0\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "    for sentence in abstracts.tolist():\n",
    "        clear_output(wait=True)\n",
    "        index += 1\n",
    "        sen_emb = get_scibert_embedding(scibert_model, scibert_tokenizer, sentence)\n",
    "        embeddings.append(sen_emb)\n",
    "\n",
    "        stop = timeit.default_timer()\n",
    "\n",
    "        if (index/length*100) < 1:\n",
    "            expected_time = \"Calculating...\"\n",
    "\n",
    "        else:\n",
    "            time_perc = timeit.default_timer()\n",
    "            expected_time = np.round( (time_perc-start) /(index/length) /60,2)\n",
    "\n",
    "        print(index, length)\n",
    "        print(expected_time)\n",
    "    return embeddings\n",
    "        \n",
    "# Find bloom embeddings\n",
    "def get_bloom_embeddings(abstracts):\n",
    "    # Load scibert\n",
    "    bloom_tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-350m\")\n",
    "    bloom_model = BloomModel.from_pretrained(\"bigscience/bloom-350m\")\n",
    "\n",
    "    print('bloom_tokenizer is type:', type(bloom_tokenizer))\n",
    "    print('bloom_model is type:', type(bloom_model))\n",
    "    \n",
    "    embeddings = []\n",
    "    length = len(abstracts.tolist())\n",
    "    index = 0\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "    for sentence in abstracts.tolist():\n",
    "        clear_output(wait=True)\n",
    "        index += 1\n",
    "        sen_emb = get_bloom_embedding(bloom_model, bloom_tokenizer, sentence)\n",
    "        embeddings.append(sen_emb)\n",
    "\n",
    "        stop = timeit.default_timer()\n",
    "\n",
    "        if (index/length*100) < 1:\n",
    "            expected_time = \"Calculating...\"\n",
    "\n",
    "        else:\n",
    "            time_perc = timeit.default_timer()\n",
    "            expected_time = np.round( (time_perc-start) /(index/length) /60,2)\n",
    "\n",
    "        print(index, length)\n",
    "        print(expected_time)\n",
    "    return embeddings\n",
    "\n",
    "# Function to get document embedding\n",
    "def get_scibert_embedding(model, tokenizer, text):\n",
    "\n",
    "    # Encode with special tokens ([CLS] and [SEP], returning pytorch tensors\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,\n",
    "                        truncation=True,\n",
    "                        max_length=512,\n",
    "                        add_special_tokens = True,\n",
    "                        return_tensors = 'pt'\n",
    "                )\n",
    "    input_ids = encoded_dict['input_ids']  \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()   \n",
    "    \n",
    "    # Run through BERT\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        # Extract hidden states\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    # Select the embeddings\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "    # Calculate average of token vectors\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    # Convert to np array\n",
    "    sentence_embedding = sentence_embedding.detach().numpy()\n",
    "    print(\"sentence embedding shape:\", sentence_embedding.shape)\n",
    "\n",
    "    return sentence_embedding\n",
    "\n",
    "def get_bloom_embedding(model, tokenizer, text):\n",
    "\n",
    "    # Encode with special tokens ([CLS] and [SEP], returning pytorch tensors\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,\n",
    "                        truncation=True,\n",
    "                        max_length=512,\n",
    "                        add_special_tokens = True,\n",
    "                        return_tensors = 'pt'\n",
    "                )\n",
    "    input_ids = encoded_dict['input_ids']  \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()   \n",
    "    \n",
    "    # Run through Bloom\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        # Extract hidden states\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "\n",
    "    # Select the word embeddings on the last layer\n",
    "    token_vecs = last_hidden_states[0]\n",
    "    print(\"Token vecs:\", token_vecs.shape)\n",
    "    # Calculate average of token vectors/word embeddings\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    # Convert to np array\n",
    "    sentence_embedding = sentence_embedding.detach().numpy()\n",
    "    print(\"sentence embedding shape:\", sentence_embedding.shape)\n",
    "\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embeddings(name, method):\n",
    "    # Parse XML\n",
    "    abstractsInclude, tagsInclude = parseXML(name + '/' + name + 'Include.xml', 1)\n",
    "    abstractsExclude, tagsExclude = parseXML(name + '/' + name + 'Exclude.xml', 0)\n",
    "    df = pd.DataFrame(list(zip(tagsInclude + tagsExclude, abstractsInclude + abstractsExclude)), columns =['code', 'abstract'])\n",
    "\n",
    "    if method == \"simple\":\n",
    "        df['embeddings'] = get_simple_embeddings(df['abstract'])\n",
    "    if method == \"scibert\":\n",
    "        df['embeddings'] = get_scibert_embeddings(df['abstract'])\n",
    "    if method == \"bloom-350m\":\n",
    "        df = df.iloc[:10]\n",
    "        df['embeddings'] = get_bloom_embeddings(df['abstract'][:10])\n",
    "    \n",
    "    # Save dataframe to prevent recalculation\n",
    "    df.to_pickle(\"./\" + name + \"/\" + name + \"-embeddings-\" + method + \".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token vecs: torch.Size([116, 1024])\n",
      "sentence embedding shape: (1024,)\n",
      "10 10\n",
      "0.35\n"
     ]
    }
   ],
   "source": [
    "# method - Select from [\"simple\", \"scibert\", \"bloom\"]\n",
    "method = \"bloom-350m\"\n",
    "\n",
    "# names = [\"cellulitis\", \"copper\", \"search\", \"uti\", \"overdiagnosis\"]\n",
    "names = [\"copper\"]\n",
    "\n",
    "for name in names:\n",
    "    calculate_embeddings(name, method)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "system",
   "language": "python",
   "name": "system"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
